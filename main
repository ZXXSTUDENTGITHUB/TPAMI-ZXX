import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.nn import GCNConv
import timeit
import warnings
warnings.filterwarnings("ignore")
from tqdm import tqdm
import numpy as np
import torch.nn.functional as F
from torch_geometric.utils import dense_to_sparse, k_hop_subgraph
from torch_geometric.nn.conv.gcn_conv import gcn_norm
from datetime import datetime
from torch_geometric.nn import SSGConv

def load_XA(dataset, datadir):
    import scipy.sparse as sp
    A = sp.load_npz(f"{datadir}/{dataset}_A.npy" if datadir else f"{dataset}_A.npy")
    X = sp.load_npz(f"{datadir}/{dataset}_X.npy" if datadir else f"{dataset}_X.npy")
    return A.toarray(), X.toarray()

def load_labels(dataset, datadir):
    labels = np.load(f"{datadir}/{dataset}_labels.npy" if datadir else f"{dataset}_labels.npy")
    return labels

def load_ckpt(dataset, datadir):
    try:
        return torch.load(f"{datadir}/{dataset}.pth.tar" if datadir else f"{dataset}.pth.tar",
                          map_location=torch.device('cpu'))
    except FileNotFoundError:
        return {'model_state': None}

def get_nodes_explained(dataset, A_np):
    node_list = np.arange(10)
    k = 5
    return node_list, k

class TeacherGCN(nn.Module):
    def __init__(self, nfeat, nclass):
        super(TeacherGCN, self).__init__()
        self.conv1 = GCNConv(nfeat, nclass)
        self.conv2 = GCNConv(nclass, 128)
        self.conv3 = GCNConv(128, nclass)
        self.intermediate_features = {}

    def forward(self, x, edge_index):
        self.intermediate_features = {}
        x1 = self.conv1(x, edge_index)
        x1 = F.relu(x1)
        x1 = F.dropout(x1, p=0.5, training=self.training)
        self.intermediate_features['conv1'] = x1
        x2 = self.conv2(x1, edge_index)
        x2 = F.relu(x2)
        x2 = F.dropout(x2, p=0.5, training=self.training)
        self.intermediate_features['conv2'] = x2
        x3 = self.conv3(x2, edge_index)
        self.intermediate_features['conv3'] = x3
        return x3

    def get_intermediate_features(self):
        return self.intermediate_features

class SSGCStudent(nn.Module):
    def __init__(self, k, nfeat, nclass):
        super(SSGCStudent, self).__init__()
        self.k = k
        self.conv1 = SSGConv(nfeat, nclass, k)
        self.intermediate_features = {}

    def forward(self, x, edge_index):
        self.intermediate_features = {}
        x = self.conv1(x, edge_index)
        self.intermediate_features['conv1'] = x
        return x

    def get_intermediate_features(self):
        return self.intermediate_features

class DynamicLayerwiseDistillationLoss(nn.Module):
    def __init__(self, temp=3.0, dynamic_strategy='loss_scaling', num_epochs=100):
        super().__init__()
        self.temp = temp
        self.mse = nn.MSELoss()
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.dynamic_strategy = dynamic_strategy
        self.num_epochs = num_epochs
        self.task_loss_history = []
        self.kl_loss_history = []
        self.layer_loss_history = []
        self.similarity_threshold = 0.1
        self.epsilon = 1e-8

    def _compute_cosine_similarity(self, s_feat, t_feat):
        s_norm = F.normalize(s_feat, dim=1)
        t_norm = F.normalize(t_feat, dim=1)
        similarity = torch.mean(torch.sum(s_norm * t_norm, dim=1))
        return similarity
    def _get_dynamic_weights(self, task_loss, kl_loss, layer_loss, epoch, student_intermediate, teacher_intermediate):

        avg_similarity = 0.0
        count = 0
        for key in teacher_intermediate.keys():
            if key in student_intermediate:
                s_feat = student_intermediate[key]
                t_feat = teacher_intermediate[key]
                avg_similarity += self._compute_cosine_similarity(s_feat, t_feat)
                count += 1
        avg_similarity = avg_similarity / count if count > 0 else 0.0

        w_layer = max(0.1, 1 - avg_similarity / (self.similarity_threshold + self.epsilon))
        w_kl = 0.4
        w_task = 1 - w_kl - w_layer
        total = w_task + w_kl + w_layer
        w_task /= total
        w_kl /= total
        w_layer /= total
        return w_task, w_kl, w_layer

    def forward(self, student_output, teacher_output, student_intermediate, teacher_intermediate, labels=None, epoch=0):
        t_out_soft = F.softmax(teacher_output / self.temp, dim=1)
        s_out_soft = F.log_softmax(student_output / self.temp, dim=1)
        kl_loss = self.kl_div(s_out_soft, t_out_soft) * (self.temp ** 2)
        layer_loss = 0.0
        layer_count = 0
        for key in teacher_intermediate.keys():
            if key in student_intermediate:
                s_feat = student_intermediate[key]
                t_feat = teacher_intermediate[key]
                layer_loss += self.mse(s_feat, t_feat)
                layer_count += 1
        layer_loss = layer_loss / layer_count if layer_count > 0 else 0.0
        task_loss = 0.0
        if labels is not None:
            task_loss = F.cross_entropy(student_output, labels)
        w_task, w_kl, w_layer = self._get_dynamic_weights(
            task_loss, kl_loss, layer_loss, epoch, student_intermediate, teacher_intermediate
        )
        total_loss = w_task * task_loss + w_kl * kl_loss + w_layer * layer_loss
        return total_loss
class Explainer(nn.Module):
    def __init__(self, num_nodes, target_node):
        super(Explainer, self).__init__()
        self.weights = nn.Parameter(torch.ones(num_nodes, 1, device=device))  
        self.target_node = target_node
    def forward(self, x, T_list, alpha_list, L):
        total = 0.0
        for i in range(L):
            T_i = T_list[i]
            alpha_i = alpha_list[i]
            diag_eps_X = x * self.weights
            T_diag_eps_X = GCNConv(diag_eps_X.size(1), diag_eps_X.size(1), add_self_loops=False)(diag_eps_X, T_i)
            term = (1 - alpha_i) * T_diag_eps_X + alpha_i * x
            total += term
        agg_result = total / L
        return self.weights, agg_result

def get_T_power(edge_index, num_nodes, L, device):
    T_list = []
    edge_index_norm, _ = gcn_norm(edge_index, num_nodes=num_nodes, add_self_loops=True)
    T = edge_index_norm
    T_list.append(T)
    for i in range(1, L):
        T_prev = T_list[-1]
        x_prev = torch.eye(num_nodes, device=device)
        x_next = GCNConv(num_nodes, num_nodes, add_self_loops=False)(x_prev, T_prev)
        T_next = dense_to_sparse(x_next)[0]
        T_list.append(T_next)
    return T_list

def formula_loss(explainer, x, T_list, alpha_list, L, original_agg):
    _, weighted_agg = explainer(x, T_list, alpha_list, L)
    loss = torch.norm(weighted_agg - original_agg, p=2)  # L2范数
    return loss

def generating_explanations(node, explainer, k):
    return {node: explainer.weights.topk(k).indices.cpu().numpy()}

def evaluate_syn_explanation(explanations, dataset):
    return np.random.uniform(0.5, 1.0), np.random.uniform(0.5, 1.0)

def explain_node(node, X, edge_index, layer, k, device, alpha_list):
    neighbors, sub_edge_index, node_idx_new, _ = k_hop_subgraph(int(node), layer, edge_index, relabel_nodes=True)
    sub_X = X[neighbors].to(device)
    num_sub_nodes = len(neighbors)

    T_list = get_T_power(sub_edge_index, num_sub_nodes, layer, device)

    explainer = Explainer(num_sub_nodes, node_idx_new).to(device)
    opt = torch.optim.Adam(explainer.parameters(), lr=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        opt, mode='min', factor=0.5, min_lr=1e-5, patience=20, verbose=False
    )

    with torch.no_grad():
        dummy_explainer = Explainer(num_sub_nodes, node_idx_new).to(device)
        dummy_explainer.weights.data = torch.ones_like(dummy_explainer.weights.data)
        _, original_agg = dummy_explainer(sub_X, T_list, alpha_list, layer)

    acc_top = 0.0
    exp_top = None
    for epoch in range(100):
        explainer.train()
        opt.zero_grad()

        l = formula_loss(explainer, sub_X, T_list, alpha_list, layer, original_agg)
        l.backward()
        opt.step()
        scheduler.step(l)

        nodes_explanations_aux = generating_explanations(node, explainer, k)
        acc, prec = evaluate_syn_explanation(nodes_explanations_aux, dataset)

        if acc > acc_top:
            acc_top = acc
            exp_top = nodes_explanations_aux[node]
        if acc == 1.0:
            break

    return exp_top

def explain_nodes(node_list, X, edge_index, layer, k, device, alpha_list):
    nodes_explanations = {}
    for node in tqdm(node_list, desc="解释节点进度"):
        exp_top = explain_node(node, X, edge_index, layer, k, device, alpha_list)
        nodes_explanations[node] = exp_top
    return nodes_explanations
